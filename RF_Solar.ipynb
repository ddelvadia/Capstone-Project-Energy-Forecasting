{"cells":[{"metadata":{},"cell_type":"markdown","source":"# https://towardsdatascience.com/train-test-split-and-cross-validation-in-python-80b61beca4b6"},{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import math\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Using Skicit-learn to split data into training and testing sets\nfrom sklearn.model_selection import train_test_split\n# Import the model we are using\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn import linear_model\nfrom sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error, explained_variance_score\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn.model_selection import cross_val_score, cross_validate, cross_val_predict\nfrom sklearn.linear_model import Ridge, RidgeCV\nfrom sklearn.linear_model import ElasticNet, ElasticNetCV\nfrom sklearn.linear_model import LassoCV\nfrom sklearn import tree\nfrom sklearn import metrics \nfrom sklearn.preprocessing import quantile_transform\nfrom sklearn.preprocessing import QuantileTransformer\nimport xgboost as xgb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# load solararray complete data using pandas\nfeatures=pd.read_csv('../input/df_solararray_complete.csv')\nfeatures.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Unarmed: 0 and Location variables are not useful since one if index and another is just name, so let's remove them"},{"metadata":{"trusted":true},"cell_type":"code","source":"features=features.drop(['Unnamed: 0', 'Location'], axis=1)\nnames=list(features.columns)\nfeatures.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# labels are the values we want to predict\nlabels=np.array(features['Electricity_KW_HR'])\n\n# Remove the labels from the features\n# axis 1 refers to the columns\nfeatures=features.drop('Electricity_KW_HR',axis=1)\n\n# saving feature names for later use\nfeature_list=list(features.columns)\n\n#convert to numpy array\nfeatures=np.array(features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split the data into training and testing sets\ntrain_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size = 0.20, random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's review the shape of each features\nprint('Training Features Shape:', train_features.shape)\nprint('Training Labels Shape:', train_labels.shape)\nprint('Testing Features Shape:', test_features.shape)\nprint('Testing Labels Shape:', test_labels.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def metric_CV():\n    # The r2_squared, rmse, mse, and explained variance based on the output of the cross validation\n    test_r2 = round(scores['test_r2'].mean(), 3)\n    test_rmse = round(math.sqrt(-scores['test_neg_mean_squared_error'].mean()), 3)\n    test_mse = round((-scores['test_neg_mean_squared_error'].mean()), 3)\n    test_exp_var_mean, test_exp_var_std = round(scores['test_explained_variance'].mean(), 3), round(scores['test_explained_variance'].std()*2, 3)\n    test_MAE=round((-scores['test_neg_mean_absolute_error'].mean()), 3)\n    \n    print('With CV Metrics:',\n                  '\\nTest R-squared:\\t\\t\\t', test_r2,\n                  '\\nTest RMSE:\\t\\t\\t', test_rmse,\n                  '\\nTest MSE:\\t\\t\\t', test_mse,\n                  '\\nTest Explained Variance:\\t {0} (+/- {1})'.format(test_exp_var_mean, test_exp_var_std),\n                  '\\nTest MAE:\\t\\t\\t', test_MAE)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**MODELS WITHOUT TRANSFORMATION**"},{"metadata":{"trusted":true},"cell_type":"code","source":"################ Linear Regression ######################\n# Create linear regression object\nlm = linear_model.LinearRegression()\n\n# Train the model using the training sets\nlm.fit(train_features, train_labels)\n\n# Make predictions using the testing set\npredictions = lm.predict(test_features)\npred_trn=lm.predict(train_features)\n\n# The coefficients\n#print('Coefficients: \\n', regr.coef_)\n# The mean squared error\n#print(\"Mean squared error: %.2f\"\n#      % mean_squared_error(test_labels, prediction))\n# Explained variance score: 1 is perfect prediction\n#print('Variance score: %.2f' % r2_score(test_labels, prediction))\n\ntest_r2 = round(r2_score(test_labels, predictions), 3)\ntrain_r2 = round(lm.score(train_features, train_labels), 3)\nrmse = round(np.sqrt(mean_squared_error(test_labels, predictions)),5)\nrmse_trn = round(np.sqrt(mean_squared_error(train_labels, pred_trn)),5)\nmse = round(mean_squared_error(test_labels, predictions), 5)\nmse_trn = round(mean_squared_error(train_labels, pred_trn), 5)\nexp_var = round(explained_variance_score(test_labels, predictions), 3)\nexp_var_trn = round(explained_variance_score(train_labels, pred_trn), 3)\n\nexplained_variance_score(train_labels, pred_trn)\nprint('\\nLinear Regression Model Metrics:',\n                  '\\n\\nTest R-squared:\\t\\t', test_r2,\n                  '\\nTrain R-squared:\\t', train_r2,\n                  '\\nRMSE:\\t\\t\\t', rmse,\n                  '\\nTrain RMSE:\\t\\t', rmse_trn,\n                  '\\nMSE:\\t\\t\\t', mse,\n                  '\\nTrain MSE:\\t\\t', mse_trn,\n                  '\\nExplained Variance:\\t', exp_var,\n                  '\\nTrain Explained Variance:', exp_var_trn)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"################ Linear Regression with Cross Validation ######################\n# Create linear regression object\nscores = cross_validate(lm, features, labels, cv = 10, scoring=['r2','neg_mean_squared_error','neg_mean_absolute_error', 'explained_variance'])\n#scores.keys()\n#predictions = cross_val_predict(lm, features, test_labels, cv=10)\n\nprint('Linear Regression:')\nmetric_CV()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Let's see Feature ranking with recursive feature elimination."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Feature ranking with recursive feature elimination.\nfrom sklearn.feature_selection import RFE\nselector = RFE(lm, 5, step=1)\nselector = selector.fit(train_features, train_labels)\nselector.n_features_\nselector.support_\nselector.ranking_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Based on the feature selection with linear model most important features are\n['Year',**'Month'**,'Day',**'Hour'**,**'Cloud_Cover_Fraction'**,'Dew_Point',**'Humidity_Fraction'**,'Precipitation','Pressure','Temperature','Visibility','Wind_Speed',**'Solar_Elevation'**]"},{"metadata":{"trusted":true},"cell_type":"code","source":"# select those columns with high importance\nselected_features=pd.DataFrame(train_features)\nselected_features=selected_features.iloc[:,[1,3,4,6,12]].values\n#selected_features\n\ntest_features=pd.DataFrame(test_features)\nselected_test_features=test_features.iloc[:,[1,3,4,6,12]].values\n#test_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train the model using the selected feature as the training sets\nlm.fit(selected_features, train_labels)\n\n# Make predictions using the testing set\npredictions = lm.predict(test_features)\npred_trn=lm.predict(selected_features)\n\ntest_r2 = round(r2_score(test_labels, predictions), 3)\ntrain_r2 = round(lm.score(selected_features, train_labels), 3)\nrmse = round(np.sqrt(mean_squared_error(test_labels, predictions)),5)\nrmse_trn = round(np.sqrt(mean_squared_error(train_labels, pred_trn)),5)\nmse = round(mean_squared_error(test_labels, predictions), 5)\nmse_trn = round(mean_squared_error(train_labels, pred_trn), 5)\nexp_var = round(explained_variance_score(test_labels, predictions), 3)\nexp_var_trn = round(explained_variance_score(train_labels, pred_trn), 3)\n\nexplained_variance_score(train_labels, pred_trn)\nprint('\\nLinear Regression Model Metrics:',\n                  '\\n\\nTest R-squared:\\t\\t', test_r2,\n                  '\\nTrain R-squared:\\t', train_r2,\n                  '\\nRMSE:\\t\\t\\t', rmse,\n                  '\\nTrain RMSE:\\t\\t', rmse_trn,\n                  '\\nMSE:\\t\\t\\t', mse,\n                  '\\nTrain MSE:\\t\\t', mse_trn,\n                  '\\nExplained Variance:\\t', exp_var,\n                  '\\nTrain Explained Variance:', exp_var_trn)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"################### Lasso Regression Model ########################################\nlasso = linear_model.Lasso(alpha=10, max_iter=2000)\nlasso.fit(train_features, train_labels)\n \n# Make predictions using the testing set\npredictions = lasso.predict(test_features)\npred_trn=lasso.predict(train_features)\n\ntest_r2 = round(r2_score(test_labels, predictions), 3)\ntrain_r2 = round(lasso.score(train_features, train_labels), 3)\nrmse = round(np.sqrt(mean_squared_error(test_labels, predictions)),5)\nrmse_trn = round(np.sqrt(mean_squared_error(train_labels, pred_trn)),5)\nmse = round(mean_squared_error(test_labels, predictions), 5)\nmse_trn = round(mean_squared_error(train_labels, pred_trn), 5)\nexp_var = round(explained_variance_score(test_labels, predictions), 3)\nexp_var_trn = round(explained_variance_score(train_labels, pred_trn), 3)\n\nexplained_variance_score(train_labels, pred_trn)\nprint('Lasso Regression Model Metrics:',\n                  '\\n\\nTest R-squared:\\t\\t', test_r2,\n                  '\\nTrain R-squared:\\t', train_r2,\n                  '\\nRMSE:\\t\\t\\t', rmse,\n                  '\\nTrain RMSE:\\t\\t', rmse_trn,\n                  '\\nMSE:\\t\\t\\t', mse,\n                  '\\nTrain MSE:\\t\\t', mse_trn,\n                  '\\nExplained Variance:\\t', exp_var,\n                  '\\nTrain Explained Variance:', exp_var_trn)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"################### Lasso Regression Model with Cross Validation ########################################\nlasso_CV = LassoCV(cv=10, random_state=0).fit(features, labels)\n#lasso_CV.score(train_features, train_labels) \nscores = cross_validate(lasso_CV, features, labels, cv = 10, scoring=['r2','neg_mean_squared_error','neg_mean_absolute_error', 'explained_variance'])\n#print(scores.mean()) \nscores.keys()\n\nprint('Lasso Regression:')\nmetric_CV()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"################### Ridge Regression ##################################\nrdg = Ridge(alpha=1.0)\nrdg.fit(train_features, train_labels) \n\n# Make predictions\npredictions=rdg.predict(test_features)\npred_trn=lm.predict(train_features)\n\n# calculate metrics\ntest_r2 = round(r2_score(test_labels, predictions), 3)\ntrain_r2 = round(rdg.score(train_features, train_labels), 3)\nrmse = round(np.sqrt(mean_squared_error(test_labels, predictions)),3)\nrmse_trn = round(np.sqrt(mean_squared_error(train_labels, pred_trn)),3)\nmse = round(mean_squared_error(test_labels, predictions), 3)\nmse_trn = round(mean_squared_error(train_labels, pred_trn), 3)\nexp_var = round(explained_variance_score(test_labels, predictions), 3)\nexp_var_trn = round(explained_variance_score(train_labels, pred_trn), 3)\n\nexplained_variance_score(train_labels, pred_trn)\nprint('\\nLinear Regression Model Metrics:',\n                  '\\n\\nTest R-squared:\\t\\t', test_r2,\n                  '\\nTrain R-squared:\\t', train_r2,\n                  '\\nRMSE:\\t\\t\\t', rmse,\n                  '\\nTrain RMSE:\\t\\t', rmse_trn,\n                  '\\nMSE:\\t\\t\\t', mse,\n                  '\\nTrain MSE:\\t\\t', mse_trn,\n                  '\\nExplained Variance:\\t', exp_var,\n                  '\\nTrain Explained Variance:', exp_var_trn)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"################## Ridge Regression with Cross Validation ####################\nrdg_CV = RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1]).fit(features, labels)\n#rdg_CV.score(features, labels) \n\n# cross validation score evaluation. specifically the test values of each of the 10 cross fold\nscores = cross_validate(rdg_CV, features, labels, cv = 10, scoring=['r2','neg_mean_squared_error','neg_mean_absolute_error', 'explained_variance'])\n#print(scores.mean()) \nscores.keys()\n\nprint('Ridge Regression:')\nmetric_CV()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"################## ElasticNet Model #######################\nEN = ElasticNet(random_state=0)\nEN.fit(train_features, train_labels)\npredictions=EN.predict(test_features)\n\n# Make predictions\npredictions=EN.predict(test_features)\npred_trn=EN.predict(train_features)\n\n# calculate metrics\ntest_r2 = round(r2_score(test_labels, predictions), 3)\ntrain_r2 = round(EN.score(train_features, train_labels), 3)\nrmse = round(np.sqrt(mean_squared_error(test_labels, predictions)),3)\nrmse_trn = round(np.sqrt(mean_squared_error(train_labels, pred_trn)),3)\nmse = round(mean_squared_error(test_labels, predictions), 3)\nmse_trn = round(mean_squared_error(train_labels, pred_trn), 3)\nexp_var = round(explained_variance_score(test_labels, predictions), 3)\nexp_var_trn = round(explained_variance_score(train_labels, pred_trn), 3)\n\n#explained_variance_score(train_labels, pred_trn)\nprint('\\nLinear Regression Model Metrics:',\n                  '\\n\\nTest R-squared:\\t\\t', test_r2,\n                  '\\nTrain R-squared:\\t', train_r2,\n                  '\\nRMSE:\\t\\t\\t', rmse,\n                  '\\nTrain RMSE:\\t\\t', rmse_trn,\n                  '\\nMSE:\\t\\t\\t', mse,\n                  '\\nTrain MSE:\\t\\t', mse_trn,\n                  '\\nExplained Variance:\\t', exp_var,\n                  '\\nTrain Explained Variance:', exp_var_trn)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"################## ElasticNet Regression Model with Cross Validation #######################\nEN_CV = ElasticNetCV(cv=10, random_state=0).fit(features, labels)\n\n# cross validation score evaluation. specifically the test values of each of the 10 cross fold\nscores = cross_validate(EN_CV, features, labels, cv = 10, scoring=['r2','neg_mean_squared_error','neg_mean_absolute_error', 'explained_variance'])\n#print(scores.mean()) \nscores.keys()\n\nprint('Elastic Net Regression:')\nmetric_CV()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"############### Decision Tree model ######################\ndtr = tree.DecisionTreeRegressor()\ndtr.fit(train_features, train_labels)\n\n# Make predictions\npredictions=dtr.predict(test_features)\npred_trn=dtr.predict(train_features)\n\n# calculate metrics\ntest_r2 = round(r2_score(test_labels, predictions), 3)\ntrain_r2 = round(dtr.score(train_features, train_labels), 3)\nrmse = round(np.sqrt(mean_squared_error(test_labels, predictions)),3)\nrmse_trn = round(np.sqrt(mean_squared_error(train_labels, pred_trn)),3)\nmse = round(mean_squared_error(test_labels, predictions), 3)\nmse_trn = round(mean_squared_error(train_labels, pred_trn), 3)\nexp_var = round(explained_variance_score(test_labels, predictions), 3)\nexp_var_trn = round(explained_variance_score(train_labels, pred_trn), 3)\n\n#explained_variance_score(train_labels, pred_trn)\nprint('\\nDecision Tree Regression Model Metrics:',\n                  '\\n\\nTest R-squared:\\t\\t', test_r2,\n                  '\\nTrain R-squared:\\t', train_r2,\n                  '\\nRMSE:\\t\\t\\t', rmse,\n                  '\\nTrain RMSE:\\t\\t', rmse_trn,\n                  '\\nMSE:\\t\\t\\t', mse,\n                  '\\nTrain MSE:\\t\\t', mse_trn,\n                  '\\nExplained Variance:\\t', exp_var,\n                  '\\nTrain Explained Variance:', exp_var_trn)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"################## Decision Tree Model with Cross Validation #######################\ndtr = tree.DecisionTreeRegressor()\ndtr.fit(features, labels)\n\n# cross validation score evaluation. specifically the test values of each of the 10 cross fold\nscores = cross_validate(dtr, features, labels, cv = 10, scoring=['r2','neg_mean_squared_error','neg_mean_absolute_error', 'explained_variance'])\n#print(scores.mean()) \nscores.keys()\n\nprint('Decision Tree Model:')\nmetric_CV()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"############# Random Forest Model #################\n# Instantiate model with 100 decision trees\nrfr = RandomForestRegressor(n_estimators = 100, random_state = 42)\n# Train the model on training data\nrfr.fit(train_features, train_labels)\n\n# Use the forest's predict method on the test data\npredictions = rfr.predict(test_features)\npred_trn=rfr.predict(train_features)\n\n\ntest_r2 = round(r2_score(test_labels, predictions), 3)\ntrain_r2 = round(rfr.score(train_features, train_labels), 3)\nrmse = round(np.sqrt(mean_squared_error(test_labels, predictions)),5)\nrmse_trn = round(np.sqrt(mean_squared_error(train_labels, pred_trn)),5)\nmse = round(mean_squared_error(test_labels, predictions), 5)\nmse_trn = round(mean_squared_error(train_labels, pred_trn), 5)\nexp_var = round(explained_variance_score(test_labels, predictions), 3)\nexp_var_trn = round(explained_variance_score(train_labels, pred_trn), 3)\n\nexplained_variance_score(train_labels, pred_trn)\nprint('Lasso Regression Model Metrics:',\n                  '\\n\\nTest R-squared:\\t\\t', test_r2,\n                  '\\nTrain R-squared:\\t', train_r2,\n                  '\\nRMSE:\\t\\t\\t', rmse,\n                  '\\nTrain RMSE:\\t\\t', rmse_trn,\n                  '\\nMSE:\\t\\t\\t', mse,\n                  '\\nTrain MSE:\\t\\t', mse_trn,\n                  '\\nExplained Variance:\\t', exp_var,\n                  '\\nTrain Explained Variance:', exp_var_trn)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# scatter plot of the test labels vs predictions (without cross_val_predict)\n\n# plot\np2=sns.regplot(x=test_labels, y=predictions, line_kws={\"color\":\"r\",\"alpha\":0.7,\"lw\":5})\n#plt.scatter(test_labels, predictions)\n\n#p1=sns.regplot(x=test_labels, y=predictions, line_kws={\"color\":\"r\",\"alpha\":0.7,\"lw\":5})\np2.set(xlabel='Test Labels', ylabel='Predicted Labels', title='Random Forest Model: Test Vs Predicted Labels')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########################## RANDOM FOREST WITH CROSS_VALIDATION #########################################\nrfr.cv = RandomForestRegressor(n_estimators=100, random_state = 42)\n#scores = cross_val_score(RFR, train_features, train_labels, cv = 10, scoring='r2')\nscores = cross_validate(rfr.cv, features, labels, cv = 10, scoring=['r2','neg_mean_squared_error','neg_mean_absolute_error', 'explained_variance'])\nscores.keys()\n#print(scores.mean()) \n#predictions = cross_val_predict(rfr.cv, test_features, test_labels, cv=10)\n\nprint('Random Forest Model:')\nmetric_CV()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# cross validation score evaluation. specifically the test values of each of the 10 cross fold\ntest_r2 = round(scores['test_r2'].mean(), 3)\ntrain_r2 = round(scores['train_r2'].mean(), 3)\nrmse = round(np.sqrt(-scores['test_neg_mean_squared_error'].mean()),5)\nmae = round(-scores['test_neg_mean_absolute_error'].mean(), 5)\nexp_var = round(scores['test_explained_variance'].mean(), 3)\n            \nprint('\\nDecision Tree Model Metrics:',\n                  '\\n\\nTrain R-squared:\\t', train_r2,\n                  '\\nTest R-squared:\\t\\t', test_r2,\n                  '\\nRMSE:\\t\\t\\t', rmse,\n                  '\\nMAE:\\t\\t\\t', mae,\n                  '\\nExplained Variance:\\t', exp_var)\n\n\n\n\n# This method is using the test labels and train features to find the cross vallue r2 for test and train, \n# The rmse, mae, and explained variance score on test and prediction \ntest_r2 = round(r2_score(test_labels, predictions), 3)\n# Train the model on training data\nrfr.cv.fit(train_features, train_labels)\ntrain_r2 = round(rfr.cv.score(train_features, train_labels), 3)\nrmse = round(np.sqrt(mean_squared_error(test_labels, predictions)),5)\nmae = round(mean_absolute_error(test_labels, predictions), 5)\nexp_var = round(explained_variance_score(test_labels, predictions), 3)\n            \nprint('\\nRandom Forest Regressor Model Metrics:',\n                  '\\n\\nTrain R-squared:\\t', train_r2,\n                  '\\nTest R-squared:\\t', test_r2,\n                  '\\nRMSE:\\t', rmse,\n                  '\\nMAE:\\t', mae,\n                  '\\nExplained Variance:\\t', exp_var)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# scatter plot of the test labels vs predictions (from cross_val_predict)\n# scatter plot of the test labels vs predictions (without cross_val_predict)\n\n# plot\nsns.regplot(x=test_labels, y=predictions, line_kws={\"color\":\"r\",\"alpha\":0.7,\"lw\":5})\nplt.scatter(test_labels, predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"################# Random Forest with Randomized grid search ####################\nfrom sklearn.model_selection import RandomizedSearchCV\n\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 100, stop = 1000, num = 10)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\nscoring=['r2','neg_mean_squared_error','neg_mean_absolute_error', 'explained_variance']\n\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\n\nprint(random_grid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use the random grid to search for best hyperparameters\n# First create the base model to tune\nrfr = RandomForestRegressor(random_state = 42)\n# Random search of parameters, using 3 fold cross validation, \n# search across 100 different combinations, and use all available cores\nrfr_random = RandomizedSearchCV(estimator=rfr, param_distributions=random_grid,\n                              n_iter = 100, scoring='neg_mean_absolute_error', \n                              cv = 3, verbose=2, random_state=42, n_jobs=-1,\n                              return_train_score=True)\n\n# Fit the random search model\nrfr_random.fit(train_features, train_labels);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_random = rfr_random.best_estimator_\nbest_random\n#rfr_random.best_params_\npredictions = best_random.predict(test_features)\n\n# Instantiate model with 100 decision trees\n#rfr_best = RandomForestRegressor(n_estimators = 100, random_state = 42)\n\n# Train the model on training data\n#rfr.fit(train_features, train_labels)\n\n# Use the forest's predict method on the test data\n#predictions = rfr.predict(test_features)\n#pred_trn=rfr.predict(train_features)\n\ntest_r2 = round(r2_score(test_labels, predictions), 3)\ntrain_r2 = round(best_random.score(train_features, train_labels), 3)\nrmse = round(np.sqrt(mean_squared_error(test_labels, predictions)),5)\nrmse_trn = round(np.sqrt(mean_squared_error(train_labels, pred_trn)),5)\nmae = round(mean_absolute_error(test_labels, predictions), 5)\nmae_trn = round(mean_absolute_error(train_labels, pred_trn), 5)\nexp_var = round(explained_variance_score(test_labels, predictions), 3)\nexp_var_trn = round(explained_variance_score(train_labels, pred_trn), 3)\n\nprint('\\nRandom Forest Regressor Model Metrics:',\n                  '\\n\\nTest R-squared:\\t\\t', test_r2,\n                  '\\nTrain R-squared:\\t', train_r2,\n                  '\\nRMSE:\\t\\t\\t', rmse,\n                  '\\nTrain RMSE:\\t\\t', rmse_trn,\n                  '\\nMAE:\\t\\t\\t', mae,\n                  '\\nTrain MAE:\\t\\t', mae_trn,\n                  '\\nExplained Variance:\\t', exp_var,\n                  '\\nTrain Explained Variance:', exp_var_trn)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = model\npred_trn=rfr.predict(train_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#############################  Support Vector Machine ##################################\nfrom sklearn.svm import SVR\n#n_samples, n_features = 10, 5\n#rng = np.random.RandomState(0)\nclf = SVR(gamma='scale', C=1.0, epsilon=0.1)\n#clf = SVR(kernel='linear')\n# Train the model on training data\nclf.fit(train_features, train_labels) \n\n\n\n# Use the forest's predict method on the test data\npredictions = clf.predict(test_features)\npred_trn=clf.predict(train_features)\n\n\ntest_r2 = round(r2_score(test_labels, predictions), 3)\ntrain_r2 = round(clf.score(train_features, train_labels), 3)\nrmse = round(np.sqrt(mean_squared_error(test_labels, predictions)),5)\nrmse_trn = round(np.sqrt(mean_squared_error(train_labels, pred_trn)),5)\nmse = round(mean_squared_error(test_labels, predictions), 5)\nmse_trn = round(mean_squared_error(train_labels, pred_trn), 5)\nexp_var = round(explained_variance_score(test_labels, predictions), 3)\nexp_var_trn = round(explained_variance_score(train_labels, pred_trn), 3)\n\nexplained_variance_score(train_labels, pred_trn)\nprint('Random Forest Regression Model Metrics:',\n                  '\\n\\nTest R-squared:\\t\\t', test_r2,\n                  '\\nTrain R-squared:\\t', train_r2,\n                  '\\nRMSE:\\t\\t\\t', rmse,\n                  '\\nTrain RMSE:\\t\\t', rmse_trn,\n                  '\\nMSE:\\t\\t\\t', mse,\n                  '\\nTrain MSE:\\t\\t', mse_trn,\n                  '\\nExplained Variance:\\t', exp_var,\n                  '\\nTrain Explained Variance:', exp_var_trn)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########################## SUPPORT VECTOR MACHINE WITH CROSS_VALIDATION #########################################\n#clf_cv = RandomForestRegressor(n_estimators=100, random_state = 42)\n#scores = cross_val_score(RFR, train_features, train_labels, cv = 10, scoring='r2')\nscores = cross_validate(clf, train_features, train_labels, cv = 10, scoring=['r2','neg_mean_squared_error','neg_mean_absolute_error', 'explained_variance'])\nscores.keys()\n#print(scores.mean()) \n#predictions = cross_val_predict(rfr.cv, test_features, test_labels, cv=10)\n\nprint('Support Vector Machine Model:')\nmetric_CV()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"#############################################################################################################################\n#############################################################################################################################\n##  REPEAT THIS MODEL WITH NORMALIZED MODEL FOR IMPROVEMENT\n#############################################################################################################################\n#############################################################################################################################"},{"metadata":{"trusted":true},"cell_type":"code","source":"# normalize using Yeo Johnson:\n#pt = PowerTransformer()\n\n\n# load data using pandas\n#features=pd.read_csv('C:/Users/Dhaval/Documents/CSC 672/Final_Project_Dhaval_Delvadia/2015contest_CSV/new_data/df_solararray_complete.csv')\nfeatures=pd.read_csv('../input/df_solararray_complete.csv')\nfeatures.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# note: we only need to normalize continuous variables\ncontinuous_features = features.iloc[:,2:16]\ncontinuous_features.head()\nnames=list(continuous_features.columns)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#pt.fit(continuous_features)\n#pt.transform(continuous_features)\n\n#names=features.columns\n#df_norm = pd.DataFrame(pt.transform(continuous_features), columns=names)\n#df_norm.head(10)\n\ndf = pd.DataFrame(continuous_features, columns=names)\ndf.head(10)\n\n\n# labels are the values we want to predict\nlabels=np.array(df['Electricity_KW_HR'])\n\n# Remove the labels from the features\n# axis 1 refers to the columns\nfeatures=df.drop('Electricity_KW_HR',axis=1)\n\n# saving feature names for later use\nfeatures_list=list(features.columns)\n\n#convert to numpy array\nfeatures=np.array(features)\n\n# normalize using Yeo-Johnson:\npt3 = PowerTransformer()\nx_t3=pt3.fit_transform(features)\npt4 = PowerTransformer()\ny_t4=pt4.fit_transform(labels.reshape(-1,1))\n\n# Split the data into training and testing sets\ntrain_features, test_features, train_labels, test_labels = train_test_split(x_t3, y_t4, test_size = 0.20, random_state = 42)\n\n# let's review the shape of each features\nprint('Training Features Shape:', train_features.shape)\nprint('Training Labels Shape:', train_labels.shape)\nprint('Testing Features Shape:', test_features.shape)\nprint('Testing Labels Shape:', test_labels.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# labels are the values we want to predict\n#labels_norm=np.array(df_norm['Electricity_KW_HR'])\n\n# Remove the labels from the features axis 1 refers to the columns\n#features_norm=df_norm.drop('Electricity_KW_HR',axis=1)\n\n# saving feature names for later use\n#features_norm_list=list(features_norm.columns)\n\n#convert to numpy array\n#features_norm=np.array(features_norm)\n\n# Split the data into training and testing sets\n#train_features, test_features, train_labels, test_labels = train_test_split(features_norm, labels_norm, test_size = 0.20, random_state = 42)\n\n# let's review the shape of each features\n# print('Training Features Shape:', train_features.shape)\n# print('Training Labels Shape:', train_labels.shape)\n# print('Testing Features Shape:', test_features.shape)\n# print('Testing Labels Shape:', test_labels.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"################ Linear Regression ######################\n# Create linear regression object\nlm = linear_model.LinearRegression()\n\n# Train the model using the training sets\nlm.fit(train_features, train_labels)\n\n# Make predictions using the testing set\npredictions = lm.predict(test_features)\npred_trn=lm.predict(train_features)\n\n\ntest_r2 = round(r2_score(test_labels, predictions), 3)\ntrain_r2 = round(lm.score(train_features, train_labels), 3)\nrmse = round(np.sqrt(mean_squared_error(test_labels, predictions)),5)\nrmse_trn = round(np.sqrt(mean_squared_error(train_labels, pred_trn)),5)\nmse = round(mean_squared_error(test_labels, predictions), 5)\nmse_trn = round(mean_squared_error(train_labels, pred_trn), 5)\nexp_var = round(explained_variance_score(test_labels, predictions), 3)\nexp_var_trn = round(explained_variance_score(train_labels, pred_trn), 3)\nmae= round(mean_absolute_error(predictions,test_labels), 5)\nmae_trn=round(mean_absolute_error(pred_trn,train_labels), 5)\n\nexplained_variance_score(train_labels, pred_trn)\nprint('\\nLinear Regression Model Metrics:',\n                  '\\n\\nTest R-squared:\\t\\t', test_r2,\n                  '\\nTrain R-squared:\\t', train_r2,\n                  '\\nRMSE:\\t\\t\\t', rmse,\n                  '\\nTrain RMSE:\\t\\t', rmse_trn,\n                  '\\nMSE:\\t\\t\\t', mse,\n                  '\\nTrain MSE:\\t\\t', mse_trn,\n                  '\\nExplained Variance:\\t', exp_var,\n                  '\\nTrain Explained Variance:', exp_var_trn,\n                  '\\nMAE:\\t\\t\\t', mae,\n                  '\\nTrain MAE:\\t\\t', mae_trn)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"################ Linear Regression with Cross Validation ######################\n# Create linear regression object\nscores = cross_validate(lm, x_t3, y_t4, cv = 10, scoring=['r2','neg_mean_squared_error','neg_mean_absolute_error', 'explained_variance'])\n#scores.keys()\n#predictions = cross_val_predict(lm, features, test_labels, cv=10)\n\nprint('Linear Regression:')\nmetric_CV()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores.keys()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"################### Lasso Regression Model ########################################\nlasso = linear_model.Lasso(alpha=1, max_iter=100)\nlasso.fit(train_features, train_labels)\n \n# Make predictions using the testing set\npredictions = lasso.predict(test_features)\npred_trn=lasso.predict(train_features)\n\ntest_r2 = round(r2_score(test_labels, predictions), 3)\ntrain_r2 = round(lasso.score(train_features, train_labels), 3)\nrmse = round(np.sqrt(mean_squared_error(test_labels, predictions)),5)\nrmse_trn = round(np.sqrt(mean_squared_error(train_labels, pred_trn)),5)\nmse = round(mean_squared_error(test_labels, predictions), 5)\nmse_trn = round(mean_squared_error(train_labels, pred_trn), 5)\nexp_var = round(explained_variance_score(test_labels, predictions), 3)\nexp_var_trn = round(explained_variance_score(train_labels, pred_trn), 3)\nmae= round(mean_absolute_error(predictions,test_labels), 5)\nmae_trn=round(mean_absolute_error(pred_trn,train_labels), 5)\n\n\n\nexplained_variance_score(train_labels, pred_trn)\nprint('Lasso Regression Model Metrics:',\n                  '\\n\\nTest R-squared:\\t\\t', test_r2,\n                  '\\nTrain R-squared:\\t', train_r2,\n                  '\\nRMSE:\\t\\t\\t', rmse,\n                  '\\nTrain RMSE:\\t\\t', rmse_trn,\n                  '\\nMSE:\\t\\t\\t', mse,\n                  '\\nTrain MSE:\\t\\t', mse_trn,\n                  '\\nExplained Variance:\\t', exp_var,\n                  '\\nTrain Explained Variance:', exp_var_trn,\n                  '\\nMAE:\\t\\t\\t', mae,\n                  '\\nTrain MAE:\\t\\t', mae_trn)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"################### Lasso Regression Model with Cross Validation ########################################\nlasso_CV = LassoCV(cv=10, random_state=0).fit(train_features, train_labels)\n#lasso_CV.score(train_features, train_labels) \nscores = cross_validate(lasso_CV, x_t3, y_t4, cv = 10, scoring=['r2','neg_mean_squared_error','neg_mean_absolute_error', 'explained_variance'])\n#print(scores.mean()) \nscores.keys()\n\nprint('Lasso Regression:')\nmetric_CV()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"################### Ridge Regression ##################################\nrdg = Ridge(alpha=1.0)\nrdg.fit(train_features, train_labels) \n\n# Make predictions\npredictions=rdg.predict(test_features)\npred_trn=lm.predict(train_features)\n\n# calculate metrics\ntest_r2 = round(r2_score(test_labels, predictions), 3)\ntrain_r2 = round(rdg.score(train_features, train_labels), 3)\nrmse = round(np.sqrt(mean_squared_error(test_labels, predictions)),3)\nrmse_trn = round(np.sqrt(mean_squared_error(train_labels, pred_trn)),3)\nmse = round(mean_squared_error(test_labels, predictions), 3)\nmse_trn = round(mean_squared_error(train_labels, pred_trn), 3)\nexp_var = round(explained_variance_score(test_labels, predictions), 3)\nexp_var_trn = round(explained_variance_score(train_labels, pred_trn), 3)\nmae= round(mean_absolute_error(predictions,test_labels), 5)\nmae_trn=round(mean_absolute_error(pred_trn,train_labels), 5)\n\n\nexplained_variance_score(train_labels, pred_trn)\nprint('Ridge Regression Model Metrics:',\n                  '\\n\\nTest R-squared:\\t\\t', test_r2,\n                  '\\nTrain R-squared:\\t', train_r2,\n                  '\\nRMSE:\\t\\t\\t', rmse,\n                  '\\nTrain RMSE:\\t\\t', rmse_trn,\n                  '\\nMSE:\\t\\t\\t', mse,\n                  '\\nTrain MSE:\\t\\t', mse_trn,\n                  '\\nExplained Variance:\\t', exp_var,\n                  '\\nTrain Explained Variance:', exp_var_trn,\n                  '\\nMAE:\\t\\t\\t', mae,\n                  '\\nTrain MAE:\\t\\t', mae_trn)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"################## Ridge Regression with Cross Validation ####################\n#rdg_CV = RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1]).fit(features_norm, labels_norm)\n#rdg_CV.score(features, labels) \nrdg_CV = RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1]).fit(x_t3, y_t4)\n\n# cross validation score evaluation. specifically the test values of each of the 10 cross fold\n#scores = cross_validate(rdg_CV, features_norm, labels_norm, cv = 10, scoring=['r2','neg_mean_squared_error','neg_mean_absolute_error', 'explained_variance'])\n#print(scores.mean())\nscores = cross_validate(rdg_CV, x_t3, y_t4, cv = 10, scoring=['r2','neg_mean_squared_error','neg_mean_absolute_error', 'explained_variance'])\nscores.keys()\n\nprint('Ridge Regression:')\nmetric_CV()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"################## ElasticNet Model #######################\nEN = ElasticNet(random_state=0)\nEN.fit(train_features, train_labels)\npredictions=EN.predict(test_features)\n\n# Make predictions\npredictions=EN.predict(test_features)\npred_trn=EN.predict(train_features)\n\n# calculate metrics\ntest_r2 = round(r2_score(test_labels, predictions), 3)\ntrain_r2 = round(EN.score(train_features, train_labels), 3)\nrmse = round(np.sqrt(mean_squared_error(test_labels, predictions)),3)\nrmse_trn = round(np.sqrt(mean_squared_error(train_labels, pred_trn)),3)\nmse = round(mean_squared_error(test_labels, predictions), 3)\nmse_trn = round(mean_squared_error(train_labels, pred_trn), 3)\nexp_var = round(explained_variance_score(test_labels, predictions), 3)\nexp_var_trn = round(explained_variance_score(train_labels, pred_trn), 3)\nmae= round(mean_absolute_error(predictions,test_labels), 5)\nmae_trn=round(mean_absolute_error(pred_trn,train_labels), 5)\n\n#explained_variance_score(train_labels, pred_trn)\nprint('\\nLinear Regression Model Metrics:',\n                  '\\n\\nTest R-squared:\\t\\t', test_r2,\n                  '\\nTrain R-squared:\\t', train_r2,\n                  '\\nRMSE:\\t\\t\\t', rmse,\n                  '\\nTrain RMSE:\\t\\t', rmse_trn,\n                  '\\nMSE:\\t\\t\\t', mse,\n                  '\\nTrain MSE:\\t\\t', mse_trn,\n                  '\\nExplained Variance:\\t', exp_var,\n                  '\\nTrain Explained Variance:', exp_var_trn,\n                  '\\nMAE:\\t\\t\\t', mae,\n                  '\\nTrain MAE:\\t\\t', mae_trn)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_t4.ravel()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"################## ElasticNet Regression Model with Cross Validation #######################\n#EN_CV = ElasticNetCV(cv=10, random_state=0).fit(features_norm, labels_norm)\nEN_CV = ElasticNetCV(cv=10, random_state=0).fit(x_t3, y_t4.ravel())\n\n# cross validation score evaluation. specifically the test values of each of the 10 cross fold\n#scores = cross_validate(EN_CV, features_norm, labels_norm, cv = 10, scoring=['r2','neg_mean_squared_error','neg_mean_absolute_error', 'explained_variance'])\n#print(scores.mean()) \nscores = cross_validate(EN_CV, x_t3, y_t4.ravel(), cv = 10, scoring=['r2','neg_mean_squared_error','neg_mean_absolute_error', 'explained_variance'])\nscores.keys()\n\nprint('Elastic Net Regression:')\nmetric_CV()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"############### Decision Tree model ######################\ndtr = tree.DecisionTreeRegressor()\ndtr.fit(train_features, train_labels)\n\n# Make predictions\npredictions=dtr.predict(test_features)\npred_trn=dtr.predict(train_features)\n\n# calculate metrics\ntest_r2 = round(r2_score(test_labels, predictions), 3)\ntrain_r2 = round(dtr.score(train_features, train_labels), 3)\nrmse = round(np.sqrt(mean_squared_error(test_labels, predictions)),3)\nrmse_trn = round(np.sqrt(mean_squared_error(train_labels, pred_trn)),3)\nmse = round(mean_squared_error(test_labels, predictions), 3)\nmse_trn = round(mean_squared_error(train_labels, pred_trn), 3)\nexp_var = round(explained_variance_score(test_labels, predictions), 3)\nexp_var_trn = round(explained_variance_score(train_labels, pred_trn), 3)\nmae= round(mean_absolute_error(predictions,test_labels), 5)\nmae_trn=round(mean_absolute_error(pred_trn,train_labels), 5)\n\n#explained_variance_score(train_labels, pred_trn)\nprint('\\nDecision Tree Regression Model Metrics:',\n                  '\\n\\nTest R-squared:\\t\\t', test_r2,\n                  '\\nTrain R-squared:\\t', train_r2,\n                  '\\nRMSE:\\t\\t\\t', rmse,\n                  '\\nTrain RMSE:\\t\\t', rmse_trn,\n                  '\\nMSE:\\t\\t\\t', mse,\n                  '\\nTrain MSE:\\t\\t', mse_trn,\n                  '\\nExplained Variance:\\t', exp_var,\n                  '\\nTrain Explained Variance:', exp_var_trn,\n                  '\\nMAE:\\t\\t\\t', mae,\n                  '\\nTrain MAE:\\t\\t', mae_trn)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"################## Decision Tree Model with Cross Validation #######################\ndtr = tree.DecisionTreeRegressor()\n#dtr.fit(features_norm, labels_norm)\ndtr.fit(x_t3, y_t4)\n\n# cross validation score evaluation. specifically the test values of each of the 10 cross fold\n#scores = cross_validate(dtr, features_norm, labels_norm, cv = 10, scoring=['r2','neg_mean_squared_error','neg_mean_absolute_error', 'explained_variance'])\nscores = cross_validate(dtr, x_t3, y_t4, cv = 10, scoring=['r2','neg_mean_squared_error','neg_mean_absolute_error', 'explained_variance'])\n\n#print(scores.mean()) \nscores.keys()\n\nprint('Decision Tree Model:')\nmetric_CV()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"############# Random Forest Model #################\n# Instantiate model with 100 decision trees\nrfr = RandomForestRegressor(n_estimators = 100, random_state = 42)\n# Train the model on training data\nrfr.fit(train_features, train_labels)\n\n# Use the forest's predict method on the test data\npredictions = rfr.predict(test_features)\npred_trn=rfr.predict(train_features)\n\n\ntest_r2 = round(r2_score(test_labels, predictions), 3)\ntrain_r2 = round(rfr.score(train_features, train_labels), 3)\nrmse = round(np.sqrt(mean_squared_error(test_labels, predictions)),5)\nrmse_trn = round(np.sqrt(mean_squared_error(train_labels, pred_trn)),5)\nmse = round(mean_squared_error(test_labels, predictions), 5)\nmse_trn = round(mean_squared_error(train_labels, pred_trn), 5)\nexp_var = round(explained_variance_score(test_labels, predictions), 3)\nexp_var_trn = round(explained_variance_score(train_labels, pred_trn), 3)\nmae= round(mean_absolute_error(predictions,test_labels), 5)\nmae_trn=round(mean_absolute_error(pred_trn,train_labels), 5)\n\nexplained_variance_score(train_labels, pred_trn)\nprint('Random Forest Regression Model Metrics:',\n                  '\\n\\nTest R-squared:\\t\\t', test_r2,\n                  '\\nTrain R-squared:\\t', train_r2,\n                  '\\nRMSE:\\t\\t\\t', rmse,\n                  '\\nTrain RMSE:\\t\\t', rmse_trn,\n                  '\\nMSE:\\t\\t\\t', mse,\n                  '\\nTrain MSE:\\t\\t', mse_trn,\n                  '\\nExplained Variance:\\t', exp_var,\n                  '\\nTrain Explained Variance:', exp_var_trn,\n                  '\\nMAE:\\t\\t\\t', mae,\n                  '\\nTrain MAE:\\t\\t', mae_trn)\n\n\nfrom pprint import pprint\n# Look at parameters used by our current forest\nprint('Parameters currently in use:\\n')\npprint(rfr.get_params())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Features sorted by their score:\")\nprint(sorted(zip(map(lambda x: round(x, 4), rfr.feature_importances_), names), reverse=True))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# scatter plot of the test labels vs predictions (without cross_val_predict)\n\nfeatures=pd.read_csv('../input/df_solararray_complete.csv')\n# note: we only need to normalize continuous variables\ncontinuous_features = features.iloc[:,2:16]\ncontinuous_features.head()\nnames=list(continuous_features.columns)\n\ndf = pd.DataFrame(continuous_features, columns=names)\ndf.head(10)\n\n# labels are the values we want to predict\nlabels=np.array(df['Electricity_KW_HR'])\n\n# Remove the labels from the features\n# axis 1 refers to the columns\nfeatures=df.drop('Electricity_KW_HR',axis=1)\n\n# saving feature names for later use\nfeatures_list=list(features.columns)\n\n#convert to numpy array\nfeatures=np.array(features)#\n\n# Split the data into training and testing sets\ntrain_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size = 0.20, random_state = 42)\n#test_labels=np.array(test_labels)\n\n\nx=pd.DataFrame(pt4.inverse_transform(predictions.reshape(-1,1)))\n#x\n#test_labels=pt4.inverse_transform(test_labels.reshape(-1,1))\nimport seaborn as sns\n# plot\nsns.regplot(x=test_labels, y=x.iloc[:,0], line_kws={\"color\":\"r\",\"alpha\":0.7,\"lw\":5})\n#plt.scatter(test_labels, x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x.iloc[:,0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"p4=sns.regplot(x=test_labels, y=x.iloc[:,0], line_kws={\"color\":\"r\",\"alpha\":0.7,\"lw\":5})\np4.set(xlabel='Test Labels', ylabel='Predicted Labels', title='Random Forest Transformed: Test Vs Predicted Labels')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x[:,0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax=plt.plot(test_labels, alpha=0.9, color='orange')\nax=plt.plot(x.iloc[:,0], alpha=0.7, color='g')\nplt.xlabel('index')\nplt.ylabel('Electricity_KW_HR')\nplt.title('Actual vs. Predicted Electricity_KW_HR')\n\nimport matplotlib.patches as mpatches\nimport matplotlib.pyplot as plt\nplt.rcParams[\"figure.figsize\"] = [30,15]\ngreen_patch = mpatches.Patch(color='orange', label='Actual Electricity_KW_HR')\norange_patch = mpatches.Patch(color='g', label='Predicted Electricity_KW_HR by RF')\n#plt.legend(handles=[green_patch, orange_patch], bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n#plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\nplt.legend(handles=[green_patch, orange_patch], loc='center left', bbox_to_anchor=(1, 0.5))\n#plt.figure(figsize=(10,12))\n\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets plot the output of actual vs predicted for first 100 values instead of all as shown above.\n\nax=plt.plot(test_labels[0:100], alpha=0.8, color='orange')\nax=plt.plot(x.iloc[:100,0], alpha=0.7, color='g')\nplt.xlabel('index')\nplt.ylabel('Electricity_KW_HR')\nplt.title('Actual vs. Predicted Electricity_KW_HR')\n\nimport matplotlib.patches as mpatches\nimport matplotlib.pyplot as plt\nplt.rcParams[\"figure.figsize\"] = [30,15]\ngreen_patch = mpatches.Patch(color='orange', label='Actual Electricity_KW_HR')\norange_patch = mpatches.Patch(color='g', label='Predicted Electricity_KW_HR by RF')\n#plt.legend(handles=[green_patch, orange_patch], bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n#plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\nplt.legend(handles=[green_patch, orange_patch], loc='center left', bbox_to_anchor=(1, 0.5))\n#plt.figure(figsize=(10,12))\n\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########################## RANDOM FOREST WITH CROSS_VALIDATION #########################################\nrfr.cv = RandomForestRegressor(n_estimators=100, random_state = 42)\n#scores = cross_val_score(RFR, train_features, train_labels, cv = 10, scoring='r2')\n#scores = cross_validate(rfr.cv, features, labels, cv = 10, scoring=['r2','neg_mean_squared_error','neg_mean_absolute_error', 'explained_variance'])\nscores = cross_validate(rfr.cv, x_t3, y_t4, cv = 10, scoring=['r2','neg_mean_squared_error','neg_mean_absolute_error', 'explained_variance'])\nscores.keys()\n#print(scores.mean()) \n#predictions = cross_val_predict(rfr.cv, test_features, test_labels, cv=10)\n\nprint('Random Forest Model:')\nmetric_CV()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# scatter plot of the test labels vs predictions (from cross_val_predict)\n# scatter plot of the test labels vs predictions (without cross_val_predict)\nimport seaborn as sns\n# plot\np1=sns.regplot(x=test_labels, y=predictions, line_kws={\"color\":\"r\",\"alpha\":0.7,\"lw\":5})\np1.set(xlabel='Test Labels', ylabel='Predicted Labels', title='Random Forest CV Model: Test Vs Predicted Labels')\nplt.show()\n#plt.scatter(test_labels, predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###########################  RESULT IS PASSED TO THE NEXT LINE (NO NEED TO DO THIS AGAIN)  #######################\n################# Random Forest with Randomized grid search ######################################################\nfrom sklearn.model_selection import RandomizedSearchCV\n\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\nscoring=['r2','neg_mean_squared_error','neg_mean_absolute_error', 'explained_variance']\n\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\n\npprint(random_grid)\n\n\n########## (TAKES A LONG TIME SINCE ITS A GRID SEARCH) Use the random grid to search for best hyperparameters ############\n# First create the base model to tune\nrfr = RandomForestRegressor(random_state = 42)\n# Random search of parameters, using 3 fold cross validation, \n# search across 100 different combinations, and use all available cores\nrfr_random = RandomizedSearchCV(estimator=rfr, param_distributions=random_grid,\n                              n_iter = 100, scoring='neg_mean_absolute_error', \n                              cv = 3, verbose=2, random_state=42, n_jobs=-1,\n                              return_train_score=True)\n\n# Fit the random search model\nrfr_random.fit(train_features, train_labels)\n\n# We can view the best parameters from fitting the random search:\nrfr_random.best_params_\n\n# fine tune it around the best numbers\nbest_random = rfr_random.best_estimator_\nbest_random","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Best find the predict the output\nrfr.cv = RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=70,\n           max_features='auto', max_leaf_nodes=None,\n           min_impurity_decrease=0.0, min_impurity_split=None,\n           min_samples_leaf=2, min_samples_split=2,\n           min_weight_fraction_leaf=0.0, n_estimators=1400, n_jobs=None,\n           oob_score=False, random_state=42, verbose=0)\n#scores = cross_val_score(RFR, train_features, train_labels, cv = 10, scoring='r2')\n#scores = cross_validate(rfr.cv, features, labels, cv = 10, scoring=['r2','neg_mean_squared_error','neg_mean_absolute_error', 'explained_variance'])\nscores = cross_validate(rfr.cv, x_t3, y_t4, cv = 10, scoring=['r2','neg_mean_squared_error','neg_mean_absolute_error', 'explained_variance'])\nscores.keys()\n#print(scores.mean()) \n#predictions = cross_val_predict(rfr.cv, test_features, test_labels, cv=10)\n\nprint('Random Forest Model:')\nmetric_CV()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#############################################################\n########## let's remove 'year' feature and re-run random forest again #############\n# load solararray complete data using pandas\ndf=pd.read_csv('../input/df_solararray_complete.csv')\ndf.head(3)\n\ndf=df.drop(['Unnamed: 0', 'Location', 'Year'], axis=1)\nnames=list(df.columns)\ndf.head(3)\n\n#df = pd.DataFrame(continuous_features, columns=names)\n#df.head(10)\n\n\n# labels are the values we want to predict\nlabels=np.array(df['Electricity_KW_HR'])\n\n# Remove the labels from the features\n# axis 1 refers to the columns\nfeatures=df.drop('Electricity_KW_HR',axis=1)\n\n# saving feature names for later use\nfeatures_list=list(df.columns)\n\n#convert to numpy array\nfeatures=np.array(features)\n\n# normalize using Yeo-Johnson:\npt3 = PowerTransformer()\nx_t3=pt3.fit_transform(features)\npt4 = PowerTransformer()\ny_t4=pt4.fit_transform(labels.reshape(-1,1))\n\n# Split the data into training and testing sets\ntrain_features, test_features, train_labels, test_labels = train_test_split(x_t3, y_t4, test_size = 0.20, random_state = 42)\n\n# let's review the shape of each features\nprint('Training Features Shape:', train_features.shape)\nprint('Training Labels Shape:', train_labels.shape)\nprint('Testing Features Shape:', test_features.shape)\nprint('Testing Labels Shape:', test_labels.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"############# Random Forest Model #################\n# Instantiate model with 100 decision trees\nrfr = RandomForestRegressor(n_estimators = 100, random_state = 42)\n# Train the model on training data\nrfr.fit(train_features, train_labels)\n\n# Use the forest's predict method on the test data\npredictions = rfr.predict(test_features)\npred_trn=rfr.predict(train_features)\n\n\ntest_r2 = round(r2_score(test_labels, predictions), 3)\ntrain_r2 = round(rfr.score(train_features, train_labels), 3)\nrmse = round(np.sqrt(mean_squared_error(test_labels, predictions)),5)\nrmse_trn = round(np.sqrt(mean_squared_error(train_labels, pred_trn)),5)\nmse = round(mean_squared_error(test_labels, predictions), 5)\nmse_trn = round(mean_squared_error(train_labels, pred_trn), 5)\nexp_var = round(explained_variance_score(test_labels, predictions), 3)\nexp_var_trn = round(explained_variance_score(train_labels, pred_trn), 3)\nmae= round(mean_absolute_error(predictions,test_labels), 5)\nmae_trn=round(mean_absolute_error(pred_trn,train_labels), 5)\n\nexplained_variance_score(train_labels, pred_trn)\nprint('Random Forest Regression Model Metrics:',\n                  '\\n\\nTest R-squared:\\t\\t', test_r2,\n                  '\\nTrain R-squared:\\t', train_r2,\n                  '\\nRMSE:\\t\\t\\t', rmse,\n                  '\\nTrain RMSE:\\t\\t', rmse_trn,\n                  '\\nMSE:\\t\\t\\t', mse,\n                  '\\nTrain MSE:\\t\\t', mse_trn,\n                  '\\nExplained Variance:\\t', exp_var,\n                  '\\nTrain Explained Variance:', exp_var_trn,\n                  '\\nMAE:\\t\\t\\t', mae,\n                  '\\nTrain MAE:\\t\\t', mae_trn)\n\n\nfrom pprint import pprint\n# Look at parameters used by our current forest\nprint('Parameters currently in use:\\n')\npprint(rfr.get_params())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Features sorted by their score:\")\nprint(sorted(zip(map(lambda x: round(x, 4), rfr.feature_importances_), names), reverse=True))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax=plt.plot(pd.DataFrame(pt4.inverse_transform(test_labels.reshape(-1,1))), alpha=0.2, color='blue')\nax=plt.plot(x.iloc[:,0], alpha=0.2, color='red')\nplt.xlabel('index')\nplt.ylabel('Electricity_KW_HR')\nplt.title('Actual vs. Predicted Electricity_KW_HR')\n\nimport matplotlib.patches as mpatches\nimport matplotlib.pyplot as plt\nplt.rcParams[\"figure.figsize\"] = [30,15]\ngreen_patch = mpatches.Patch(color='blue', label='Actual Electricity_KW_HR')\norange_patch = mpatches.Patch(color='red', label='Predicted Electricity_KW_HR by RF')\n#plt.legend(handles=[green_patch, orange_patch], bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n#plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\nplt.legend(handles=[green_patch, orange_patch], loc='center left', bbox_to_anchor=(1, 0.5))\n#plt.figure(figsize=(10,12))\n\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets plot the output of actual vs predicted for first 100 values instead of all as shown above.\n\nax=plt.plot(pt4.inverse_transform(test_labels.reshape(-1,1))[0:100,0], alpha=0.2, color='blue')\nax=plt.plot(x.iloc[:100,0], alpha=0.2, color='red')\nplt.xlabel('index')\nplt.ylabel('Electricity_KW_HR')\nplt.title('Actual vs. Predicted Electricity_KW_HR')\n\nimport matplotlib.patches as mpatches\nimport matplotlib.pyplot as plt\nplt.rcParams[\"figure.figsize\"] = [30,15]\ngreen_patch = mpatches.Patch(color='blue', label='Actual Electricity_KW_HR')\norange_patch = mpatches.Patch(color='red', label='Predicted Electricity_KW_HR by RF')\n#plt.legend(handles=[green_patch, orange_patch], bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n#plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\nplt.legend(handles=[green_patch, orange_patch], loc='center left', bbox_to_anchor=(1, 0.5))\n#plt.figure(figsize=(10,12))\n\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########################## RANDOM FOREST WITH CROSS_VALIDATION #########################################\nrfr.cv = RandomForestRegressor(n_estimators=100, random_state = 42)\n#scores = cross_val_score(RFR, train_features, train_labels, cv = 10, scoring='r2')\n#scores = cross_validate(rfr.cv, features, labels, cv = 10, scoring=['r2','neg_mean_squared_error','neg_mean_absolute_error', 'explained_variance'])\nscores = cross_validate(rfr.cv, x_t3, y_t4, cv = 10, scoring=['r2','neg_mean_squared_error','neg_mean_absolute_error', 'explained_variance'])\nscores.keys()\n#print(scores.mean()) \n#predictions = cross_val_predict(rfr.cv, test_features, test_labels, cv=10)\n\nprint('Random Forest Model:')\nmetric_CV()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Best find the predict the output\nrfr.cv = RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=70,\n           max_features='auto', max_leaf_nodes=None,\n           min_impurity_decrease=0.0, min_impurity_split=None,\n           min_samples_leaf=2, min_samples_split=2,\n           min_weight_fraction_leaf=0.0, n_estimators=1400, n_jobs=None,\n           oob_score=False, random_state=42, verbose=0)\n#scores = cross_val_score(RFR, train_features, train_labels, cv = 10, scoring='r2')\n#scores = cross_validate(rfr.cv, features, labels, cv = 10, scoring=['r2','neg_mean_squared_error','neg_mean_absolute_error', 'explained_variance'])\nscores = cross_validate(rfr.cv, x_t3, y_t4, cv = 10, scoring=['r2','neg_mean_squared_error','neg_mean_absolute_error', 'explained_variance'])\nscores.keys()\n#print(scores.mean()) \n#predictions = cross_val_predict(rfr.cv, test_features, test_labels, cv=10)\n\nprint('Random Forest Model:')\nmetric_CV()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#############################  Support Vector Machine ##################################\nfrom sklearn.svm import SVR\nn_samples, n_features = 10, 5\n#rng = np.random.RandomState(0)\nclf = SVR(gamma='scale', C=1.0, epsilon=0.2)\n# Train the model on training data\nclf.fit(train_features, train_labels) \n\n\n\n# Use the forest's predict method on the test data\npredictions = clf.predict(test_features)\npred_trn=clf.predict(train_features)\n\n\ntest_r2 = round(r2_score(test_labels, predictions), 3)\ntrain_r2 = round(clf.score(train_features, train_labels), 3)\nrmse = round(np.sqrt(mean_squared_error(test_labels, predictions)),5)\nrmse_trn = round(np.sqrt(mean_squared_error(train_labels, pred_trn)),5)\nmse = round(mean_squared_error(test_labels, predictions), 5)\nmse_trn = round(mean_squared_error(train_labels, pred_trn), 5)\nexp_var = round(explained_variance_score(test_labels, predictions), 3)\nexp_var_trn = round(explained_variance_score(train_labels, pred_trn), 3)\nmae= round(mean_absolute_error(predictions,test_labels), 5)\nmae_trn=round(mean_absolute_error(pred_trn,train_labels), 5)\n\nexplained_variance_score(train_labels, pred_trn)\nprint('SUPPORT VECTOR MACHINE Model Metrics:',\n                  '\\n\\nTest R-squared:\\t\\t', test_r2,\n                  '\\nTrain R-squared:\\t', train_r2,\n                  '\\nRMSE:\\t\\t\\t', rmse,\n                  '\\nTrain RMSE:\\t\\t', rmse_trn,\n                  '\\nMSE:\\t\\t\\t', mse,\n                  '\\nTrain MSE:\\t\\t', mse_trn,\n                  '\\nExplained Variance:\\t', exp_var,\n                  '\\nTrain Explained Variance:', exp_var_trn,\n                  '\\nMAE:\\t\\t\\t', mae,\n                  '\\nTrain MAE:\\t\\t', mae_trn)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########################## SUPPORT VECTOR MACHINE WITH CROSS_VALIDATION #########################################\n#clf_cv = RandomForestRegressor(n_estimators=100, random_state = 42)\n#scores = cross_val_score(RFR, train_features, train_labels, cv = 10, scoring='r2')\nscores = cross_validate(clf, x_t3, y_t4, cv = 10, scoring=['r2','neg_mean_squared_error','neg_mean_absolute_error', 'explained_variance'])\nscores.keys()\n#print(scores.mean()) \n#predictions = cross_val_predict(rfr.cv, test_features, test_labels, cv=10)\n\nprint('Support Vector Machine Model:')\nmetric_CV()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Let's read the scenario file now for prediction on those six days"},{"metadata":{"trusted":true},"cell_type":"code","source":"# load complete training data using pandas and then use the query, groupby and aggragate function\n\n#features=pd.read_csv('C:/Users/Dhaval/Documents/CSC 672/Final_Project_Dhaval_Delvadia/2015contest_CSV/new_data/df_solararray_complete.csv')\nfeatures=pd.read_csv('../input/df_solararray_complete.csv')\nfeatures.head()\nfeatures=features.drop(['Unnamed: 0', 'Location', 'Year'], axis=1)\nfeatures.head()\n# note: we only need to normalize continuous variables\n#x_train = features.iloc[:,0:12]\n#y_train = features.iloc[:,12]\n\n# we need to impute the actual values of these 6 dates to find the mean value of the Electricity_KW_HR for comparasion with predicted values for these dates\nq315=features.query('Month == 3 & Day == 15')\nq626=features.query('Month == 6 & Day == 26')\nq713=features.query('Month == 7 & Day == 3')\nq1013=features.query('Month == 10 & Day == 13')\nq1119=features.query('Month == 11 & Day == 19')\nq1225=features.query('Month == 12 & Day == 25')\n\ng315=q315.groupby(['Month','Day','Hour'])\ng626=q626.groupby(['Month','Day','Hour'])\ng713=q713.groupby(['Month','Day','Hour'])\ng1013=q1013.groupby(['Month','Day','Hour'])\ng1119=q1119.groupby(['Month','Day','Hour'])\ng1225=q1225.groupby(['Month','Day','Hour'])\n\nt315=pd.DataFrame(g315['Electricity_KW_HR'].agg(np.mean))\nt626=pd.DataFrame(g626['Electricity_KW_HR'].agg(np.mean))\nt713=pd.DataFrame(g713['Electricity_KW_HR'].agg(np.mean))\nt1013=pd.DataFrame(g1013['Electricity_KW_HR'].agg(np.mean))\nt1119=pd.DataFrame(g1119['Electricity_KW_HR'].agg(np.mean))\nt1225=pd.DataFrame(g1225['Electricity_KW_HR'].agg(np.mean))\n\nt315","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# load scenario (test file) data using pandas and then sort the values by hour to keep the date and the hours the same\nfeatures_snr=pd.read_csv('../input/scenario.csv')\nfeatures_snr.head()\nfeatures_snr=features_snr.drop(['Unnamed: 0', 'City', 'Year', 'Day_of_week', 'HolidayName', 'School_Day', 'Weekdays'], axis=1)\n#names_snr=list(features_snr.columns)\n#names_snr\n#values = {'Pressure': 998, 'Precipitation':0.0}\nfeatures_snr.fillna(np.mean, inplace=True)\n\n#features_snr.query('Month == 3 & Day == 15')\n\n\n# these are the six date values we will need for predicting\na=features_snr.query('Month == 3 & Day == 15').sort_values(by=['Hour'])\nb=features_snr.query('Month == 6 & Day == 26').sort_values(by=['Hour'])\nc=features_snr.query('Month == 7 & Day == 3').sort_values(by=['Hour'])\n#d=features_snr.query('Month == 10 & Day == 13').sort_values(by=['Hour'])\ne=features_snr.query('Month == 11 & Day == 19').sort_values(by=['Hour'])\n#f=features_snr.query('Month == 12 & Day == 25').sort_values(by=['Hour']).fillna(values)\n\n\n#test_dates_df = pd.concat([a, b,c,d,e,f])\n#test_dates_df.head()\n\n\n#x_test=test_dates_df.iloc[:,[0,1,2,4,5,6,7,8,9,10,11,3]]\n#x_test.head()\n\nfeatures_snr.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features_snr","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fit the random forest regression model without 'Location' and 'year' using Yeo-Johnson\n\n# load solararray complete data using pandas\nfeatures=pd.read_csv('../input/df_solararray_complete.csv')\nfeatures.head(3)\n\nfeatures=features.drop(['Unnamed: 0', 'Location', 'Year'], axis=1)\nnames=list(features.columns)\nfeatures.head(3)\n\n# labels are the values we want to predict\nlabels=np.array(features['Electricity_KW_HR'])\n\n# Remove the labels from the features\n# axis 1 refers to the columns\nfeatures=features.drop('Electricity_KW_HR',axis=1)\n\n# saving feature names for later use\nfeature_list=list(features.columns)\n\n#convert to numpy array\nfeatures=np.array(features)\n\n\n# Split the data into training and testing sets\n#train_X, test_X, train_y, test_y = train_test_split(features, labels, test_size = 0.20, random_state = 42)\n\n############# let's try Yeo-Johnson transformation #################\n# normalize using Yeo-Johnson:\npt9 = PowerTransformer()\npt10 = PowerTransformer()\npt11=PowerTransformer()\npt12=PowerTransformer()    \n   \nX_t=pt9.fit_transform(features) \ny_t=pt10.fit_transform(labels.reshape(-1,1))\n\na_t=pt11.fit_transform(a)\nb_t=pt11.fit_transform(b)\nc_t=pt11.fit_transform(c)\n#d_t=pt11.fit_transform(d)\ne_t=pt11.fit_transform(e)\n#f_t=pt11.fit_transform(f)\n\n\n# random forest regression model\nrfr1 = RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=70,\n           max_features='auto', max_leaf_nodes=None,\n           min_impurity_decrease=0.0, min_impurity_split=None,\n           min_samples_leaf=2, min_samples_split=2,\n           min_weight_fraction_leaf=0.0, n_estimators=1400, n_jobs=None,\n           oob_score=False, random_state=42, verbose=0)\n\n#rfr1 = RandomForestRegressor(n_estimators=1400, random_state=42)\nrfr1.fit(X_t, y_t)\n\n\n\n\n# Create linear regression object\n#lm = linear_model.LinearRegression()\n# Train the model using the training sets\n#lm.fit(train_features, train_labels)\n# Make predictions using the testing set\n#predictions = lm.predict(test_features)\n#pred_trn=lm.predict(train_features)\n\n\n# now let's predict for 6 dates\npred315=rfr1.predict(a_t)\nprediction315=pt10.inverse_transform(pred315.reshape(-1,1))\n\npred626=rfr1.predict(b_t)\nprediction626=pt10.inverse_transform(pred626.reshape(-1,1))\n\npred713=rfr1.predict(c_t)\nprediction713=pt10.inverse_transform(pred713.reshape(-1,1))\n\n#pred1013=rfr1.predict(d_t)\n#prediction1013=pt10.inverse_transform(pred1013.reshape(-1,1))\n\npred1119=rfr1.predict(e_t)\nprediction1119=pt10.inverse_transform(pred1119.reshape(-1,1))\n\n#pred1225=rfr1.predict(f_t)\n#prediction1225=pt10.inverse_transform(pred1225.reshape(-1,1))\n\nrfr1.fit(features, labels)\nprediction1013=rfr1.predict(d)\n\nrfr1.fit(features, labels)\nprediction1225=rfr1.predict(f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction315","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plot the Pred_KW_HR vs. actual KW_HR for March 15th\n\nax=plt.plot(np.array(t315['Electricity_KW_HR']), alpha=0.7, color='orange')  ######## columns may not be correct for test location\nax=plt.plot(prediction315, alpha=0.7, color='g')\nplt.xlabel('Hours')\nplt.ylabel('Electricity_KW_HR')\nplt.title('March 15th Day - Actual vs. Predicted Electricity (KWh)')\n\nimport matplotlib.patches as mpatches\nimport matplotlib.pyplot as plt\nplt.rcParams[\"figure.figsize\"] = [20,15]\ngreen_patch = mpatches.Patch(color='g', label='Predicted Electricity_KW_HR by RF')\norange_patch = mpatches.Patch(color='orange', label='Actual Electricity_KW_HR')\nplt.legend(handles=[green_patch, orange_patch], loc='center left', bbox_to_anchor=(1, 0.5))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rmse = round(np.sqrt(mean_squared_error(t315, prediction315)), 5)\nprint('RMSE between actual vs predicted for March 15th date is: ',rmse)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax=plt.plot(np.array(t626['Electricity_KW_HR']), alpha=0.7, color='orange')  ######## columns may not be correct for test location\nax=plt.plot(prediction626, alpha=0.7, color='g')\nplt.xlabel('Hours')\nplt.ylabel('Electricity_KW_HR')\nplt.title('June 26th Day - Actual vs. Predicted Electricity (KWh)')\n\nimport matplotlib.patches as mpatches\nimport matplotlib.pyplot as plt\nplt.rcParams[\"figure.figsize\"] = [20,15]\ngreen_patch = mpatches.Patch(color='orange', label='Actual Electricity_KW_HR')\norange_patch = mpatches.Patch(color='g', label='Predicted Electricity_KW_HR by RF')\nplt.legend(handles=[green_patch, orange_patch], loc='center left', bbox_to_anchor=(1, 0.5))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax=plt.plot(np.array(t713['Electricity_KW_HR']), alpha=0.7, color='orange')  ######## columns may not be correct for test location\nax=plt.plot(prediction713, alpha=0.7, color='g')\nplt.xlabel('Hours')\nplt.ylabel('Electricity_KW_HR')\nplt.title('July 13th Day - Actual vs. Predicted Electricity (KWh)')\n\nimport matplotlib.patches as mpatches\nimport matplotlib.pyplot as plt\nplt.rcParams[\"figure.figsize\"] = [20,15]\ngreen_patch = mpatches.Patch(color='orange', label='Actual Electricity_KW_HR')\norange_patch = mpatches.Patch(color='g', label='Predicted Electricity_KW_HR by RF')\nplt.legend(handles=[green_patch, orange_patch], loc='center left', bbox_to_anchor=(1, 0.5))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax=plt.plot(np.array(t1013['Electricity_KW_HR']), alpha=0.7, color='orange')  ######## columns may not be correct for test location\nax=plt.plot(prediction1013, alpha=0.7, color='g')\nplt.xlabel('Hours')\nplt.ylabel('Electricity_KW_HR')\nplt.title('october 13th Day - Actual vs. Predicted Electricity (KWh)')\n\nimport matplotlib.patches as mpatches\nimport matplotlib.pyplot as plt\nplt.rcParams[\"figure.figsize\"] = [20,15]\ngreen_patch = mpatches.Patch(color='orange', label='Actual Electricity_KW_HR')\norange_patch = mpatches.Patch(color='g', label='Predicted Electricity_KW_HR by RF')\nplt.legend(handles=[green_patch, orange_patch], loc='center left', bbox_to_anchor=(1, 0.5))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax=plt.plot(np.array(t1119['Electricity_KW_HR']), alpha=0.7, color='orange')  ######## columns may not be correct for test location\nax=plt.plot(prediction1119, alpha=0.7, color='g')\nplt.xlabel('Hours')\nplt.ylabel('Electricity_KW_HR')\nplt.title('November 19th Day - Actual vs. Predicted Electricity (KWh)')\n\nimport matplotlib.patches as mpatches\nimport matplotlib.pyplot as plt\nplt.rcParams[\"figure.figsize\"] = [20,15]\ngreen_patch = mpatches.Patch(color='orange', label='Actual Electricity_KW_HR')\norange_patch = mpatches.Patch(color='g', label='Predicted Electricity_KW_HR by RF')\nplt.legend(handles=[green_patch, orange_patch], loc='center left', bbox_to_anchor=(1, 0.5))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax=plt.plot(np.array(t1225['Electricity_KW_HR']), alpha=0.7, color='orange')  ######## columns may not be correct for test location\nax=plt.plot(prediction1225, alpha=0.7, color='g')\nplt.xlabel('Hours')\nplt.ylabel('Electricity_KW_HR')\nplt.title('Dec 25th Day - Actual vs. Predicted Electricity (KWh)')\n\nimport matplotlib.patches as mpatches\nimport matplotlib.pyplot as plt\nplt.rcParams[\"figure.figsize\"] = [20,15]\ngreen_patch = mpatches.Patch(color='orange', label='Actual Electricity_KW_HR')\norange_patch = mpatches.Patch(color='g', label='Predicted Electricity_KW_HR by RF')\nplt.legend(handles=[green_patch, orange_patch], loc='center left', bbox_to_anchor=(1, 0.5))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fit the random forest regression model without 'Location' and 'year' using Quantile Normal\n\n# load solararray complete data using pandas\nfeatures=pd.read_csv('../input/df_solararray_complete.csv')\nfeatures.head(3)\n\nfeatures=features.drop(['Unnamed: 0', 'Location', 'Year', ], axis=1)\nnames=list(features.columns)\nfeatures.head(3)\n\n# labels are the values we want to predict\nlabels=np.array(features['Electricity_KW_HR'])\n\n# Remove the labels from the features\n# axis 1 refers to the columns\nfeatures=features.drop('Electricity_KW_HR',axis=1)\n\n#fatures.query('Month==3 & Day=15')\n\n# saving feature names for later use\nfeature_list=list(features.columns)\n\n#convert to numpy array\nfeatures=np.array(features)\n\n############# let's try Quantile Normal transformation #################\nqt1=QuantileTransformer(n_quantiles=10, random_state=0)\nqt2=QuantileTransformer(n_quantiles=10, random_state=0)\nqt3=QuantileTransformer(n_quantiles=10, random_state=0)\nqt4=QuantileTransformer(n_quantiles=10, random_state=0)\nqt5=QuantileTransformer(n_quantiles=10, random_state=0)\nqt6=QuantileTransformer(n_quantiles=10, random_state=0)\nqt7=QuantileTransformer(n_quantiles=10, random_state=0)\nqt8=QuantileTransformer(n_quantiles=10, random_state=0)\n\na_t=qt1.fit_transform(a)\nb_t=qt2.fit_transform(b)\nc_t=qt3.fit_transform(c)\nd_t=qt4.fit_transform(d)\ne_t=qt5.fit_transform(e)\nf_t=qt6.fit_transform(f)\n\nX_t=qt7.fit_transform(features) \ny_t=qt8.fit_transform(labels.reshape(-1,1))\n\n\n# random forest regression model\n#rfr1 = RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=70,\n#           max_features='auto', max_leaf_nodes=None,\n#           min_impurity_decrease=0.0, min_impurity_split=None,\n#           min_samples_leaf=2, min_samples_split=2,\n#           min_weight_fraction_leaf=0.0, n_estimators=1400, n_jobs=None,\n#           oob_score=False, random_state=42, verbose=0)\n\n#random forest with quantile normalization\n#rfr2 = RandomForestRegressor(n_estimators=1400, random_state=42)\n#rfr2.fit(X_t, y_t)\n\n# Xgboost with quantile normalization\nrfr2 = xgb.XGBRegressor(objective=\"reg:linear\", random_state=42) # Xgboost model\nrfr2.fit(X_t, y_t)\n\n# now let's predict for 6 dates\npred315=rfr2.predict(a_t)\nprediction315=qt8.inverse_transform(pred315.reshape(-1,1))\n\npred626=rfr2.predict(b_t)\nprediction626=qt8.inverse_transform(pred626.reshape(-1,1))\n\npred713=rfr2.predict(c_t)\nprediction713=qt8.inverse_transform(pred713.reshape(-1,1))\n\npred1013=rfr2.predict(d_t)\nprediction1013=qt8.inverse_transform(pred1013.reshape(-1,1))\n\npred1119=rfr2.predict(e_t)\nprediction1119=qt8.inverse_transform(pred1119.reshape(-1,1))\n\npred1225=rfr2.predict(f_t)\nprediction1225=qt8.inverse_transform(pred1225.reshape(-1,1))\n\n#rfr1.fit(features, labels)\n#prediction1013=rfr1.predict(d)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plot the Pred_KW_HR vs. actual KW_HR for March 15th\n\nax=plt.plot(np.array(t315['Electricity_KW_HR']), alpha=1, color='red')  ######## columns may not be correct for test location\nax=plt.plot(prediction315, alpha=1, color='blue')\nplt.xlabel('Hours')\nplt.ylabel('Electricity_KW_HR')\nplt.title('March 15th Day - Actual vs. Predicted Electricity (KWh)')\n\nimport matplotlib.patches as mpatches\nimport matplotlib.pyplot as plt\nplt.rcParams[\"figure.figsize\"] = [20,15]\n#plt.gca().legend(('y0','y1'))\ngreen_patch = mpatches.Patch(color='Red', label='Predicted Electricity_KW_HR by RF')\norange_patch = mpatches.Patch(color='blue', label='Actual Electricity_KW_HR')\nplt.legend(handles=[green_patch, orange_patch], loc='upper right', bbox_to_anchor=(1, 0.5))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rmse = round(np.sqrt(mean_squared_error(t315, prediction315)), 5)\nprint('RMSE between actual vs predicted for March 15th date is: ',rmse)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax=plt.plot(np.array(t626['Electricity_KW_HR']), alpha=1, color='red')  ######## columns may not be correct for test location\nax=plt.plot(prediction626, alpha=1, color='blue')\nplt.xlabel('Hours')\nplt.ylabel('Electricity_KW_HR')\nplt.title('June 26th Day - Actual vs. Predicted Electricity (KWh)')\n\nimport matplotlib.patches as mpatches\nimport matplotlib.pyplot as plt\nplt.rcParams[\"figure.figsize\"] = [20,15]\ngreen_patch = mpatches.Patch(color='red', label='Actual Electricity_KW_HR')\norange_patch = mpatches.Patch(color='blue', label='Predicted Electricity_KW_HR by RF')\nplt.legend(handles=[green_patch, orange_patch], loc='best', bbox_to_anchor=(1, 0.5))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax=plt.plot(np.array(t713['Electricity_KW_HR']), alpha=1, color='red')  ######## columns may not be correct for test location\nax=plt.plot(prediction713, alpha=1, color='b')\nplt.xlabel('Hours')\nplt.ylabel('Electricity_KW_HR')\nplt.title('July 13th Day - Actual vs. Predicted Electricity (KWh)')\n\nimport matplotlib.patches as mpatches\nimport matplotlib.pyplot as plt\nplt.rcParams[\"figure.figsize\"] = [20,15]\ngreen_patch = mpatches.Patch(color='red', label='Actual Electricity_KW_HR')\norange_patch = mpatches.Patch(color='b', label='Predicted Electricity_KW_HR by RF')\nplt.legend(handles=[green_patch, orange_patch], loc='best', bbox_to_anchor=(1, 0.5))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax=plt.plot(np.array(t1013['Electricity_KW_HR']), alpha=1, color='red')  ######## columns may not be correct for test location\nax=plt.plot(prediction1013, alpha=1, color='b')\nplt.xlabel('Hours')\nplt.ylabel('Electricity_KW_HR')\nplt.title('October 13th Day - Actual vs. Predicted Electricity (KWh)')\n\nimport matplotlib.patches as mpatches\nimport matplotlib.pyplot as plt\nplt.rcParams[\"figure.figsize\"] = [20,15]\ngreen_patch = mpatches.Patch(color='red', label='Actual Electricity_KW_HR')\norange_patch = mpatches.Patch(color='b', label='Predicted Electricity_KW_HR by RF')\nplt.legend(handles=[green_patch, orange_patch], loc='best', bbox_to_anchor=(1, 0.5))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax=plt.plot(np.array(t1119['Electricity_KW_HR']), alpha=1, color='red')  ######## columns may not be correct for test location\nax=plt.plot(prediction1119, alpha=1, color='b')\nplt.xlabel('Hours')\nplt.ylabel('Electricity_KW_HR')\nplt.title('November 19th Day - Actual vs. Predicted Electricity (KWh)')\n\nimport matplotlib.patches as mpatches\nimport matplotlib.pyplot as plt\nplt.rcParams[\"figure.figsize\"] = [20,15]\ngreen_patch = mpatches.Patch(color='red', label='Actual Electricity_KW_HR')\norange_patch = mpatches.Patch(color='b', label='Predicted Electricity_KW_HR by RF')\nplt.legend(handles=[green_patch, orange_patch], loc='best', bbox_to_anchor=(1, 0.5))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax=plt.plot(np.array(t1225['Electricity_KW_HR']), alpha=1, color='red')  ######## columns may not be correct for test location\nax=plt.plot(prediction1225, alpha=1, color='b')\nplt.xlabel('Hours')\nplt.ylabel('Electricity_KW_HR')\nplt.title('Dec 25th Day - Actual vs. Predicted Electricity (KWh)')\n\nimport matplotlib.patches as mpatches\nimport matplotlib.pyplot as plt\nplt.rcParams[\"figure.figsize\"] = [20,15]\ngreen_patch = mpatches.Patch(color='red', label='Actual Electricity_KW_HR')\norange_patch = mpatches.Patch(color='b', label='Predicted Electricity_KW_HR by RF')\nplt.legend(handles=[green_patch, orange_patch], loc='best', bbox_to_anchor=(1, 0.5))\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}