{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \nfrom __future__ import absolute_import, division, print_function\n\nimport pathlib\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nimport numpy as np\n%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler, PowerTransformer\nfrom sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error, explained_variance_score\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.callbacks import EarlyStopping\nfrom keras.optimizers import Adam\nfrom keras.layers import LSTM\nfrom datetime import timedelta\nfrom sklearn.preprocessing import quantile_transform\nfrom sklearn.preprocessing import QuantileTransformer\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.\n\nprint(tf.__version__)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Use seaborn for pairplot\n!pip install -q seaborn","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Import it using pandas"},{"metadata":{"trusted":true},"cell_type":"code","source":"column_names = ['Location', 'Year', 'Month', 'Day', 'Hour', 'Cloud_Cover_Fraction',\n                'Dew_Point','Humidity_Fraction','Precipitation',\n                'Pressure', 'Temperature', 'Visibility', 'Wind_Speed',\n                'Solar_Elevation', 'Electricity_KW_HR'] \n#raw_dataset = pd.read_csv(\"../input/df_solararray_complete.csv\",\n#                      na_values = \"NA\", comment='\\t',\n#                      sep=\",\")\nraw_dataset = pd.read_csv(\"../input/df_solararray_complete.csv\")\n# , names=column_names, skipinitialspace=True\ndataset = raw_dataset.copy()\ndataset.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df=dataset.drop(['Unnamed: 0', 'Location'], axis=1)\ndf.head()\n# hour must be 0 - 23\ndf['Hour'] = df['Hour'] - 1\n\n# create datetime column\ndf['datetime'] = pd.to_datetime(df[['Year', 'Month', 'Day', 'Hour']],infer_datetime_format=True)\n\ndf = df.set_index(['datetime'],drop=True)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# may have to do something about why these values for precipitation \n# and Pressure missing. For now we will remove it since its only 5 values\ndataset.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = dataset.dropna()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset['Date']=pd.to_datetime(dataset[['Year', 'Month', 'Day', 'Hour']].rename(\n                columns={'YY': 'year', 'MM': 'month', 'DD': 'day',\n                         'hh': 'hour'}))\nn_df=dataset.iloc[:,6:16]\n#n_df.head()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Split the data into train and test\nNow split the dataset into a training set and a test set.\n\nWe will use the test set in the final evaluation of our model."},{"metadata":{"trusted":true},"cell_type":"code","source":"n_df.shape\n#n_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's split this data into train and test (80%, 20%)\ntrain_dataset = n_df.iloc[0:round(n_df.shape[0]*.8),:]\ntest_dataset = n_df.drop(train_dataset.index)\ntest_dataset.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now let's split the train_dataset into train and validation\ntrain_X = train_dataset.iloc[0:round(train_dataset.shape[0]*.8),:]\nvalid_X = train_dataset.iloc[round(train_dataset.shape[0]*.8):,:]\ntest_y = test_dataset.iloc[0:round(test_dataset.shape[0]*.8),:]\n#valid_y = \ntest_y.shape\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset.head()\ntest_dataset.head()\ntrain_dataset.shape\ntrain_dataset.head()\n\ncolNames=list(train_dataset)\ncolNames","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Look at the overall statistic"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_stats = train_dataset.describe()\ntrain_stats.pop(\"Electricity_KW_HR\")\ntrain_stats = train_stats.transpose()\ntrain_stats\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Split features from labels\nSeparate the target value, or \"label\", from the features. This label is the value that you will train the model to predict."},{"metadata":{"trusted":true},"cell_type":"code","source":"tst_Elec=test_dataset['Electricity_KW_HR']\ntst_Elec.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_labels = train_dataset.pop('Electricity_KW_HR')\ntest_labels = test_dataset.pop('Electricity_KW_HR')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Normalize the data\nLook again at the train_stats block above and note how different the ranges of each feature are.\n\nIt is good practice to normalize features that use different scales and ranges. Although the model might converge without feature normalization, it makes training more difficult, and it makes the resulting model dependent on the choice of units used in the input."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_stats['mean']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#test_dataset","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# this is z-score normalization"},{"metadata":{"trusted":true},"cell_type":"code","source":"# this is z-score normalization\ndef norm(x):\n  return (x - train_stats['mean']) / train_stats['std']\nnormed_train_data = norm(train_dataset)\nnormed_test_data = norm(test_dataset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train_labels","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# transformation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Order Normalize all data\ndef ordNorm():\n    qt1=QuantileTransformer(n_quantiles=10, random_state=0)\n    qt2=QuantileTransformer(n_quantiles=10, random_state=0)\n    qt3=QuantileTransformer(n_quantiles=10, random_state=0)\n    qt4=QuantileTransformer(n_quantiles=10, random_state=0)\n    qt5=QuantileTransformer(n_quantiles=10, random_state=0)\n    qt6=QuantileTransformer(n_quantiles=10, random_state=0)\n\n    train_X=qt1.fit_transform(train_dataset) \n    test_X=qt2.fit_transform(test_dataset) \n    train_y=qt3.fit_transform(np.array(train_labels).reshape(-1,1))\n    test_y=qt4.fit_transform(np.array(test_labels).reshape(-1,1))\n\n    #X_t=qt5.fit_transform(train_dataset) \n    #y_t=qt6.fit_transform(train_labels)\n    return train_X, train_y, test_X, test_y, qt4\n\n# normalize using Yeo Johnson transformation\ndef yeoJohnson():   \n    pt1=PowerTransformer()\n    pt2=PowerTransformer()\n    pt3=PowerTransformer()\n    pt4=PowerTransformer()\n    pt5=PowerTransformer()\n    pt6=PowerTransformer()\n\n    train_X=pt1.fit_transform(train_dataset) \n    test_X=pt2.fit_transform(test_dataset) \n    train_y=pt3.fit_transform(np.array(train_labels).reshape(-1,1))\n    test_y=pt4.fit_transform(np.array(test_labels).reshape(-1,1))\n\n    #X_t=pt5.fit_transform(features) \n    #y_t=pt6.fit_transform(labels.reshape(-1,1))\n    return train_X, train_y, test_X, test_y, qt4\n\n\n# normalize using the minMaxScaler #\ndef minMaxscaler():\n    pt1=MinMaxScaler()\n    pt2=MinMaxScaler()\n    pt3=MinMaxScaler()\n    pt4=MinMaxScaler()\n    pt5=MinMaxScaler()\n    pt6=MinMaxScaler()\n    \n    train_X=pt1.fit_transform(train_dataset) \n    test_X=pt2.fit_transform(test_dataset) \n    train_y=pt3.fit_transform(np.array(train_labels).reshape(-1,1))\n    test_y=pt4.fit_transform(np.array(test_labels).reshape(-1,1))\n\n    #X_t=pt5.fit_transform(features) \n    #y_t=pt6.fit_transform(labels.reshape(-1,1))\n    return train_X, train_y, test_X, test_y, qt4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train_X, train_y, test_X, test_y, qt4=ordNorm()\n#train_X, train_y, test_X, test_y, qt4=yeoJohnson()\ntrain_X, train_y, test_X, test_y, qt4=minMaxscaler()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(df['Solar_Elevation'], density=True, bins=30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(train_dataset.keys())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This normalized data is what we will use to train the model."},{"metadata":{},"cell_type":"markdown","source":"Build the model   \nLet's build our model. Here, we'll use a Sequential model with two densely connected hidden layers, and an output layer that returns a single, continuous value. The model building steps are wrapped in a function, build_model, since we'll create a second model, later on."},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model():\n  model = keras.Sequential([\n    layers.Dense(64, activation=tf.nn.relu, input_shape=[len(train_dataset.keys())]),\n    layers.Dense(64, activation=tf.nn.relu),\n    layers.Dense(1)\n  ])\n\n  optimizer = tf.keras.optimizers.RMSprop(0.001)\n\n  model.compile(loss='mean_squared_error',\n                optimizer=optimizer,\n                metrics=['mean_absolute_error', 'mean_squared_error'])\n  return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = build_model()\nmodel.summary() # Use the .summary method to print a simple description of the model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now try out the model. Take a batch of 10 examples from the training data and call model.predict on it."},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"#example_batch = train_dataset[:10]\n#example_result = model.predict(example_batch)\n#example_result","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems to be working, and it produces a result of the expected shape and type.\n\nTrain the model\nTrain the model for 1000 epochs, and record the training and validation accuracy in the history object."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Display training progress by printing a single dot for each completed epoch\nclass PrintDot(keras.callbacks.Callback):\n  def on_epoch_end(self, epoch, logs):\n    if epoch % 100 == 0: print('')\n    print('.', end='')\n\nEPOCHS = 100\n\n#history = model.fit(normed_train_data, train_labels, epochs=EPOCHS, validation_split = 0.2, verbose=0, callbacks=[PrintDot()])\nhistory = model.fit(train_X, train_y, epochs=EPOCHS, validation_split = 0.2, verbose=0, callbacks=[PrintDot()])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Visualize the model's training progress using the stats stored in the history object."},{"metadata":{"trusted":true},"cell_type":"code","source":"hist = pd.DataFrame(history.history)\nhist['epoch'] = history.epoch\nhist.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_history(history):\n  hist = pd.DataFrame(history.history)\n  hist['epoch'] = history.epoch\n  \n  plt.figure()\n  plt.xlabel('Epoch')\n  plt.ylabel('Mean Abs Error')\n  plt.plot(hist['epoch'], hist['mean_absolute_error'],\n           label='Train Error')\n  plt.plot(hist['epoch'], hist['val_mean_absolute_error'],\n           label = 'Val Error')\n  plt.ylim([0,0.2])\n  plt.legend()\n  \n  plt.figure()\n  plt.xlabel('Epoch')\n  plt.ylabel('Mean Square Error')\n  plt.plot(hist['epoch'], hist['mean_squared_error'],\n           label='Train Error')\n  plt.plot(hist['epoch'], hist['val_mean_squared_error'],\n           label = 'Val Error')\n  plt.ylim([0,0.1])\n  plt.legend()\n  plt.show()\n\n\nplot_history(history)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This graph shows little improvement, or even degradation in the validation error after about 100 epochs. Let's update the model.fit call to automatically stop training when the validation score doesn't improve. We'll use an EarlyStopping callback that tests a training condition for every epoch. If a set amount of epochs elapses without showing improvement, then automatically stop the training.\n\nYou can learn more about this callback here."},{"metadata":{},"cell_type":"markdown","source":"The graph shows that on the validation set, the average error is usually around +/- 2 MPG. Is this good? We'll leave that decision up to you.\n\nLet's see how well the model generalizes by using the test set, which we did not use when training the model. This tells us how well we can expect the model to predict when we use it in the real world."},{"metadata":{"trusted":true},"cell_type":"code","source":"#loss, mae, mse = model.evaluate(test_X, test_y, verbose=0)\n\n#print(\"Testing set Mean Abs Error: {:5.2f}\".format(mae))\n#print(\"Testing set Mean Square Error: {:5.2f}\".format(mse))\n\ntrain_predictions = model.predict(train_X).flatten()\ntest_predictions = model.predict(test_X).flatten()\n#print(\"The R2 score on the Train set is:\\t{:0.3f}\".format(r2_score(train_y, train_predictions)))\n#print(\"The R2 score on the Test set is:\\t{:0.3f}\".format(r2_score(test_y, test_predictions)))\n\n\ntest_r2 = round(r2_score(test_y, test_predictions), 3)\ntrain_r2 = round(r2_score(train_y, train_predictions), 3)\nrmse = round(np.sqrt(mean_squared_error(test_y, test_predictions)),5)\nrmse_trn = round(np.sqrt(mean_squared_error(train_y, train_predictions)),5)\nmse = round(mean_squared_error(test_y, test_predictions), 5)\nmse_trn = round(mean_squared_error(train_y, train_predictions), 5)\nexp_var = round(explained_variance_score(test_y, test_predictions), 3)\nexp_var_trn = round(explained_variance_score(train_y, train_predictions), 3)\nmae= round(mean_absolute_error(test_predictions,test_y), 5)\nmae_trn=round(mean_absolute_error(train_predictions,train_y), 5)\n\n#explained_variance_score(train_labels, pred_trn)\nprint('Random Forest Regression Model Metrics:',\n                  '\\n\\nTest R-squared:\\t\\t', test_r2,\n                  '\\nTrain R-squared:\\t', train_r2,\n                  '\\nRMSE:\\t\\t\\t', rmse,\n                  '\\nTrain RMSE:\\t\\t', rmse_trn,\n                  '\\nMSE:\\t\\t\\t', mse,\n                  '\\nTrain MSE:\\t\\t', mse_trn,\n                  '\\nExplained Variance:\\t', exp_var,\n                  '\\nTrain Explained Variance:', exp_var_trn,\n                  '\\nMAE:\\t\\t\\t', mae,\n                  '\\nTrain MAE:\\t\\t', mae_trn)\n\n#","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Make predictions"},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"test_predictions = model.predict(normed_test_data).flatten()\n\nplt.scatter(test_labels, test_predictions)\nplt.xlabel('True Values ')\nplt.ylabel('Predictions ')\nplt.axis('equal')\nplt.axis('square')\nplt.xlim([0,plt.xlim()[1]])\nplt.ylim([0,plt.ylim()[1]])\n_ = plt.plot([-100, 100], [-100, 100])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"test_predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"error = test_predictions - test_labels\nplt.hist(error, bins = 25)\nplt.xlabel(\"Prediction Error\")\n_ = plt.ylabel(\"Count\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It's not quite gaussian, but we might expect that because the number of samples is very small."},{"metadata":{},"cell_type":"markdown","source":"Now let's build another model with different parament for number of hidden nodes. In this case 100 notes"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Display training progress by printing a single dot for each completed epoch\n#class PrintDot(keras.callbacks.Callback):\n#  def on_epoch_end(self, epoch, logs):\n#    if epoch % 100 == 0: print('')\n#    print('.', end='')\n\n#EPOCHS = 1000\n\n#history = model1.fit(\n#  normed_train_data, train_labels,\n#  epochs=EPOCHS, validation_split = 0.2, verbose=0,\n#  callbacks=[PrintDot()])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model1():\n  model1 = keras.Sequential([\n    layers.Dense(100, activation=tf.nn.tanh, input_shape=[len(train_dataset.keys())]),\n    layers.Dense(1)\n  ])\n\n  optimizer = tf.keras.optimizers.RMSprop(0.001)\n\n  model1.compile(loss='mean_squared_error',\n                optimizer=optimizer,\n                metrics=['mean_absolute_error', 'mean_squared_error'])\n  return model1\n\nmodel1 = build_model1()\nmodel1.summary()\n\n\n# The patience parameter is the amount of epochs to check for improvement\nearly_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n\nhistory = model1.fit(normed_train_data, train_labels, epochs=EPOCHS,\n                    validation_split = 0.2, verbose=0, callbacks=[early_stop, PrintDot()])\n\n#plot_history(history)\n\n############################\nloss, mae, mse = model1.evaluate(normed_test_data, test_labels, verbose=0)\n\nprint(\"Testing set Mean Abs Error: {:5.2f}\".format(mae))\nprint(\"Testing set Mean Square Error: {:5.2f}\".format(mse))\ntrain_predictions = model1.predict(normed_train_data).flatten()\ntest_predictions = model1.predict(normed_test_data).flatten()\nprint(\"The R2 score on the Train set is:\\t{:0.3f}\".format(r2_score(train_labels, train_predictions)))\nprint(\"The R2 score on the Test set is:\\t{:0.3f}\".format(r2_score(test_labels, test_predictions)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"############### Order Normal train and Test Data #######################\n\nmodel2 = build_model()\n\n# The patience parameter is the amount of epochs to check for improvement\nearly_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n\nhistory = model2.fit(orderNorm_train_data, train_labels, epochs=EPOCHS,\n                    validation_split = 0.2, verbose=0, callbacks=[early_stop, PrintDot()])\n\nplot_history(history)\n###################################################\n\nloss, mae, mse = model2.evaluate(orderNorm_test_data, test_labels, verbose=0)\n\nprint(\"Testing set Mean Abs Error: {:5.2f}\".format(mae))\nprint(\"Testing set Mean Square Error: {:5.2f}\".format(mse))\n\ntrain_predictions = model2.predict(orderNorm_train_data).flatten()\ntest_predictions = model2.predict(orderNorm_test_data).flatten()\nprint(\"The orderNorm_train_data R2 score on the Train set is:\\t{:0.3f}\".format(r2_score(train_labels, train_predictions)))\nprint(\"The orderNorm_test_data R2 score on the Test set is:\\t{:0.3f}\".format(r2_score(test_labels, test_predictions)))\n\ntest_predictions = model2.predict(orderNorm_test_data).flatten()\n\nplt.scatter(test_labels, test_predictions)\nplt.xlabel('True Values ')\nplt.ylabel('Predictions ')\nplt.axis('equal')\nplt.axis('square')\nplt.xlim([0,plt.xlim()[1]])\nplt.ylim([0,plt.ylim()[1]])\n_ = plt.plot([-100, 100], [-100, 100])\n\nerror = test_predictions - test_labels\nplt.hist(error, bins = 25)\nplt.xlabel(\"Prediction Error\")\n_ = plt.ylabel(\"Count\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"error = test_predictions - test_labels\nplt.hist(error, bins = 25)\nplt.xlabel(\"Prediction Error\")\n_ = plt.ylabel(\"Count\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" # no normalized train and Test Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"############### no normalized train and Test Data #######################\n\nmodel3 = build_model()\n\n# The patience parameter is the amount of epochs to check for improvement\nearly_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n\nhistory = model3.fit(train_dataset, train_labels, epochs=EPOCHS,\n                    validation_split = 0.2, verbose=0, callbacks=[early_stop, PrintDot()])\n\nplot_history(history)\n###################################################\n\nloss, mae, mse = model3.evaluate(test_dataset, test_labels, verbose=0)\n\nprint(\"Testing set Mean Abs Error: {:5.2f}\".format(mae))\nprint(\"Testing set Mean Square Error: {:5.2f}\".format(mse))\n\ntrain_predictions = model3.predict(train_dataset).flatten()\ntest_predictions = model3.predict(test_dataset).flatten()\nprint(\"The train_dataset R2 score on the Train set is:\\t{:0.3f}\".format(r2_score(train_labels, train_predictions)))\nprint(\"The test_dataset R2 score on the Test set is:\\t{:0.3f}\".format(r2_score(test_labels, test_predictions)))\n\ntest_predictions = model3.predict(test_dataset).flatten()\n\nplt.scatter(test_labels, test_predictions)\nplt.xlabel('True Values ')\nplt.ylabel('Predictions ')\nplt.axis('equal')\nplt.axis('square')\nplt.xlim([0,plt.xlim()[1]])\nplt.ylim([0,plt.ylim()[1]])\n_ = plt.plot([-100, 100], [-100, 100])\n\nerror = test_predictions - test_labels\nplt.hist(error, bins = 25)\nplt.xlabel(\"Prediction Error\")\n_ = plt.ylabel(\"Count\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}